{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7deeb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "\n",
    "# LangChain bits for RAG\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"doc_path\": \"./data/Ravi_Total.docx\",\n",
    "    \"persist_dir\": \"chroma_db\",\n",
    "    \"chunk_size\": 700,\n",
    "    \"chunk_overlap\": 200,\n",
    "    \"retrieve_k\": 5,\n",
    "    \"llm_model\": \"groq/llama-3.3-70b-versatile\",\n",
    "    \"verbose\": False,  # Set to False for cleaner output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# RAG: Build / Load Knowledge Base\n",
    "# -----------------------------\n",
    "def create_knowledge_base(doc_path: str = None,\n",
    "                          persist_dir: str = None,\n",
    "                          chunk_size: int = None,\n",
    "                          chunk_overlap: int = None):\n",
    "    \"\"\"\n",
    "    Creates (or loads) a persisted Chroma DB from Ravi_Total.docx.\n",
    "    Re-running will reuse the persisted DB (no re-chunking).\n",
    "    \"\"\"\n",
    "    doc_path = doc_path or CONFIG[\"doc_path\"]\n",
    "    persist_dir = persist_dir or CONFIG[\"persist_dir\"]\n",
    "    chunk_size = chunk_size or CONFIG[\"chunk_size\"]\n",
    "    chunk_overlap = chunk_overlap or CONFIG[\"chunk_overlap\"]\n",
    "    \n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "        if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
    "            kb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "            logger.info(\"‚úÖ Loaded existing knowledge base\")\n",
    "            return kb\n",
    "\n",
    "        if not os.path.exists(doc_path):\n",
    "            raise FileNotFoundError(f\"Document not found: {doc_path}\")\n",
    "            \n",
    "        logger.info(f\"Loading document from: {doc_path}\")\n",
    "        loader = Docx2txtLoader(doc_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        if not docs:\n",
    "            raise ValueError(f\"No content found in document: {doc_path}\")\n",
    "            \n",
    "        logger.info(f\"Splitting {len(docs)} document(s) into chunks...\")\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "\n",
    "        logger.info(f\"Creating knowledge base with {len(chunks)} chunks...\")\n",
    "        kb = Chroma.from_documents(chunks, embeddings, persist_directory=persist_dir)\n",
    "        logger.info(\"‚úÖ Knowledge base created & persisted\")\n",
    "        return kb\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"‚ùå {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error creating knowledge base: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee092603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers for trace/formatting\n",
    "# -----------------------------\n",
    "def _short(text: str, n: int = 300) -> str:\n",
    "    text = text.strip().replace(\"\\n\", \" \")\n",
    "    return (text[:n] + \"‚Ä¶\") if len(text) > n else text\n",
    "\n",
    "def _doc_id(doc_text: str) -> str:\n",
    "    return hashlib.md5(doc_text.encode(\"utf-8\")).hexdigest()[:8]\n",
    "\n",
    "def retrieve_context(kb: Chroma, query: str, k: int = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve k most relevant chunks with their scores and ids.\n",
    "    \"\"\"\n",
    "    k = k or CONFIG[\"retrieve_k\"]\n",
    "    try:\n",
    "        retriever = kb.as_retriever(search_kwargs={\"k\": k})\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "        if not docs:\n",
    "            logger.warning(f\"‚ö†Ô∏è No relevant documents found for query: {query}\")\n",
    "            return []\n",
    "            \n",
    "        results = []\n",
    "        for d in docs:\n",
    "            rid = _doc_id(d.page_content)\n",
    "            results.append({\n",
    "                \"id\": rid,\n",
    "                \"excerpt\": _short(d.page_content, 420),\n",
    "                \"metadata\": d.metadata or {},\n",
    "                \"raw\": d.page_content\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error retrieving context: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def build_context_block(retrieved: List[Dict[str, Any]]) -> str:\n",
    "    if not retrieved:\n",
    "        return \"[No relevant context found]\"\n",
    "    lines = []\n",
    "    for r in retrieved:\n",
    "        lines.append(f\"[{r['id']}] {r['excerpt']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def parse_validator_output(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Expect the validator to output in this schema:\n",
    "\n",
    "    VERDICT: PASS | REWRITE\n",
    "    JUSTIFICATION:\n",
    "    - bullet 1\n",
    "    - bullet 2\n",
    "    FINAL:\n",
    "    <final hr-ready answer>\n",
    "\n",
    "    We parse this to build a structured trace.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        verdict_match = re.search(r\"^VERDICT:\\s*(.+)$\", text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        final_split = re.split(r\"\\bFINAL:\\s*\", text, flags=re.IGNORECASE)\n",
    "        just_match = re.search(r\"JUSTIFICATION:\\s*(.*)\", text, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        verdict = verdict_match.group(1).strip() if verdict_match else \"UNKNOWN\"\n",
    "        if len(final_split) >= 2:\n",
    "            final_answer = final_split[-1].strip()\n",
    "            justification_block = just_match.group(1).split(\"FINAL:\")[0].strip() if just_match else \"\"\n",
    "        else:\n",
    "            final_answer = text.strip()\n",
    "            justification_block = \"\"\n",
    "\n",
    "        bullets = []\n",
    "        for line in justification_block.splitlines():\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"-\"):\n",
    "                bullets.append(line[1:].strip())\n",
    "        return {\"verdict\": verdict, \"final_answer\": final_answer, \"bullets\": bullets}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error parsing validator output: {str(e)}\")\n",
    "        return {\"verdict\": \"ERROR\", \"final_answer\": text, \"bullets\": []}\n",
    "\n",
    "def format_trace_for_display(trace: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format trace dictionary for readable output.\"\"\"\n",
    "    output = []\n",
    "    output.append(\"\\nüìä QUERY:\")\n",
    "    output.append(f\"  {trace['query']}\\n\")\n",
    "    output.append(f\"üìö RETRIEVED CHUNKS: ({len(trace['retrieved_chunks'])} chunks)\")\n",
    "    for chunk in trace['retrieved_chunks']:\n",
    "        output.append(f\"  [{chunk['id']}] {chunk['excerpt']}\")\n",
    "    output.append(f\"\\n‚úîÔ∏è  VALIDATOR VERDICT: {trace['validator_verdict']}\")\n",
    "    if trace['validator_justification_bullets']:\n",
    "        output.append(\"üìù JUSTIFICATION:\")\n",
    "        for bullet in trace['validator_justification_bullets']:\n",
    "            output.append(f\"  ‚Ä¢ {bullet}\")\n",
    "    return \"\\n\".join(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13001df",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Crew Agents & Tasks\n",
    "# -----------------------------\n",
    "def run_hr_rag_session(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    One-run pipeline:\n",
    "      1) Retrieve context from Chroma\n",
    "      2) Answer Agent drafts answer (context-only)\n",
    "      3) Validator Agent checks/optimizes answer\n",
    "      4) Return final answer + compact reasoning trace (no chain-of-thought)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"üîç Processing query: {query}\")\n",
    "        kb = create_knowledge_base()\n",
    "        retrieved = retrieve_context(kb, query)\n",
    "        context_block = build_context_block(retrieved)\n",
    "\n",
    "        # --- Agent 1: Answer Generator (uses only provided context) ---\n",
    "        answer_agent = Agent(\n",
    "            role=\"Answer Generator\",\n",
    "            goal=\"Generate accurate answers about Ravi using ONLY the provided context.\",\n",
    "            backstory=(\n",
    "                \"You specialize in answering questions about Ravi using retrieved chunks from a private knowledge base.\"\n",
    "                \" Do not invent facts. If the context is insufficient, say 'Insufficient context'.\"\n",
    "            ),\n",
    "            llm=CONFIG[\"llm_model\"],\n",
    "            verbose=CONFIG[\"verbose\"]\n",
    "        )\n",
    "\n",
    "        task1 = Task(\n",
    "            description=(\n",
    "                \"Use ONLY the following context to answer the user's question.\\n\\n\"\n",
    "                f\"CONTEXT:\\n{context_block}\\n\\n\"\n",
    "                f\"QUESTION: {query}\\n\\n\"\n",
    "                \"REQUIREMENTS:\\n\"\n",
    "                \"- Cite chunk IDs in-line like [id] where relevant.\\n\"\n",
    "                \"- If information is not present in the context, reply exactly: 'Insufficient context'.\\n\"\n",
    "                \"- Keep answer concise and factual.\"\n",
    "            ),\n",
    "            expected_output=\"A concise factual answer with inline citations to chunk IDs where used.\",\n",
    "            agent=answer_agent\n",
    "        )\n",
    "\n",
    "        # --- Agent 2: HR Validator/Optimizer ---\n",
    "        validator_agent = Agent(\n",
    "            role=\"HR Validator\",\n",
    "            goal=(\n",
    "                \"Check if the draft is accurate to the provided context and HR-appropriate. \"\n",
    "                \"If needed, rewrite it to be HR-ready.\"\n",
    "            ),\n",
    "            backstory=(\n",
    "                \"You are a professional HR communication specialist. \"\n",
    "                \"You ensure responses are accurate, concise, and suitable for HR discussions.\"\n",
    "            ),\n",
    "            llm=CONFIG[\"llm_model\"],\n",
    "            verbose=CONFIG[\"verbose\"]\n",
    "        )\n",
    "\n",
    "        task2 = Task(\n",
    "            description=(\n",
    "                \"You will receive a DRAFT answer and the CONTEXT used to create it.\\n\\n\"\n",
    "                f\"CONTEXT:\\n{context_block}\\n\\n\"\n",
    "                \"DRAFT: {{output_of_Task_1}}\\n\\n\"\n",
    "                \"Validate against the CONTEXT and produce output in EXACTLY this format:\\n\"\n",
    "                \"VERDICT: PASS or REWRITE\\n\"\n",
    "                \"JUSTIFICATION:\\n\"\n",
    "                \"- brief bullet explaining accuracy (no internal chain-of-thought)\\n\"\n",
    "                \"- brief bullet explaining HR-clarity/tone\\n\"\n",
    "                \"FINAL:\\n\"\n",
    "                \"- If PASS: return the DRAFT verbatim\\n\"\n",
    "                \"- If REWRITE: return an improved HR-friendly version (still grounded in CONTEXT)\\n\"\n",
    "                \"\\nRULES:\\n\"\n",
    "                \"- Do NOT reveal chain-of-thought. Only provide concise bullets.\\n\"\n",
    "                \"- Do NOT add facts not present in CONTEXT.\\n\"\n",
    "                \"- Keep the final answer succinct and HR-ready.\"\n",
    "            ),\n",
    "            expected_output=\"A PASS/REWRITE verdict with brief justification bullets and the HR-ready final answer.\",\n",
    "            agent=validator_agent,\n",
    "            context=[task1]\n",
    "        )\n",
    "        \n",
    "        crew = Crew(\n",
    "            agents=[answer_agent, validator_agent],\n",
    "            tasks=[task1, task2],\n",
    "            process=Process.sequential,\n",
    "            verbose=CONFIG[\"verbose\"]\n",
    "        )\n",
    "\n",
    "        logger.info(\"‚öôÔ∏è  Running crew workflow...\")\n",
    "        result_text = crew.kickoff()\n",
    "\n",
    "        # Parse validator output into a structured result\n",
    "        parsed = parse_validator_output(result_text)\n",
    "\n",
    "        # Build a clean reasoning trace (no chain-of-thought)\n",
    "        trace = {\n",
    "            \"query\": query,\n",
    "            \"retrieved_chunks\": [\n",
    "                {\"id\": r[\"id\"], \"excerpt\": r[\"excerpt\"], \"metadata\": r[\"metadata\"]}\n",
    "                for r in retrieved\n",
    "            ],\n",
    "            \"draft_answer_note\": \"Draft generated from context-only by Answer Agent (not shown to avoid chain-of-thought).\",\n",
    "            \"validator_verdict\": parsed.get(\"verdict\", \"UNKNOWN\"),\n",
    "            \"validator_justification_bullets\": parsed.get(\"bullets\", []),\n",
    "        }\n",
    "\n",
    "        logger.info(\"‚úÖ Query processed successfully\")\n",
    "        return {\n",
    "            \"final_answer\": parsed.get(\"final_answer\", result_text),\n",
    "            \"trace\": trace\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error in HR RAG session: {str(e)}\")\n",
    "        return {\n",
    "            \"final_answer\": f\"Error processing query: {str(e)}\",\n",
    "            \"trace\": {\"query\": query, \"error\": str(e), \"retrieved_chunks\": []}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ü§ñ RAG HR ASSISTANT - Query Interface\n",
      "================================================================================\n",
      "Welcome to the HR Knowledge Base Query System!\n",
      "\n",
      "Commands:\n",
      "  ‚Ä¢ Type your question and press Enter\n",
      "  ‚Ä¢ Type 'history' to see query history\n",
      "  ‚Ä¢ Type 'help' for available commands\n",
      "  ‚Ä¢ Type 'exit' to quit\n",
      "================================================================================\n",
      "\n",
      "üí¨ Enter your question (or 'help' for commands):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 12:09:01,022 - INFO - üîç Processing query: Tell me about you\n",
      "C:\\Users\\ravik\\AppData\\Local\\Temp\\ipykernel_20584\\889055674.py:18: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Query Interface & History\n",
    "# -----------------------------\n",
    "class QueryHistory:\n",
    "    \"\"\"Manages query history and statistics.\"\"\"\n",
    "    def __init__(self, history_file: str = \"query_history.json\"):\n",
    "        self.history_file = history_file\n",
    "        self.queries = self._load_history()\n",
    "    \n",
    "    def _load_history(self) -> List[Dict[str, Any]]:\n",
    "        if os.path.exists(self.history_file):\n",
    "            try:\n",
    "                with open(self.history_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return []\n",
    "        return []\n",
    "    \n",
    "    def save_history(self):\n",
    "        with open(self.history_file, 'w') as f:\n",
    "            json.dump(self.queries, f, indent=2)\n",
    "    \n",
    "    def add_query(self, query: str, result: Dict[str, Any]):\n",
    "        entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"final_answer\": result.get(\"final_answer\", \"\"),\n",
    "            \"verdict\": result.get(\"trace\", {}).get(\"validator_verdict\", \"\")\n",
    "        }\n",
    "        self.queries.append(entry)\n",
    "        self.save_history()\n",
    "    \n",
    "    def display_history(self):\n",
    "        if not self.queries:\n",
    "            print(\"üìã No query history yet.\")\n",
    "            return\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìã QUERY HISTORY\")\n",
    "        print(\"=\"*80)\n",
    "        for i, q in enumerate(self.queries[-10:], 1):  # Show last 10\n",
    "            print(f\"\\n{i}. [{q['timestamp']}]\")\n",
    "            print(f\"   Q: {q['query']}\")\n",
    "            print(f\"   V: {q['verdict']}\")\n",
    "            print(f\"   A: {_short(q['final_answer'], 100)}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def display_welcome():\n",
    "    \"\"\"Display welcome message.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ü§ñ RAG HR ASSISTANT - Query Interface\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Welcome to the HR Knowledge Base Query System!\")\n",
    "    print(\"\\nCommands:\")\n",
    "    print(\"  ‚Ä¢ Type your question and press Enter\")\n",
    "    print(\"  ‚Ä¢ Type 'history' to see query history\")\n",
    "    print(\"  ‚Ä¢ Type 'help' for available commands\")\n",
    "    print(\"  ‚Ä¢ Type 'exit' to quit\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def display_help():\n",
    "    \"\"\"Display help information.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìö HELP - Available Commands\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\"\"\n",
    "Usage:\n",
    "  - Ask questions naturally about Ravi's background, experience, skills, etc.\n",
    "  - Questions are processed through:\n",
    "    1. Retrieval: Find relevant information from knowledge base\n",
    "    2. Generation: Create an accurate answer based on retrieved context\n",
    "    3. Validation: Ensure answer meets HR standards\n",
    "\n",
    "Commands:\n",
    "  history    - Show recent query history\n",
    "  help       - Display this help message\n",
    "  exit       - Exit the application\n",
    "\n",
    "Examples:\n",
    "  \"What is Ravi's experience with Python?\"\n",
    "  \"Summarize Ravi's work history\"\n",
    "  \"What are Ravi's key technical skills?\"\n",
    "    \"\"\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def single_run_demo():\n",
    "    \"\"\"Run a single demo query.\"\"\"\n",
    "    query = \"Summarize Ravi's Machine Learning experience for HR.\"\n",
    "    print(f\"\\nüîç Demo Query: {query}\\n\")\n",
    "    out = run_hr_rag_session(query)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ FINAL HR-READY ANSWER\")\n",
    "    print(\"=\"*80)\n",
    "    print(out[\"final_answer\"])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä REASONING TRACE\")\n",
    "    print(\"=\"*80)\n",
    "    print(format_trace_for_display(out[\"trace\"]))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def interactive_loop():\n",
    "    \"\"\"Interactive query loop with history tracking.\"\"\"\n",
    "    display_welcome()\n",
    "    history = QueryHistory()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            print(\"\\nüí¨ Enter your question (or 'help' for commands):\")\n",
    "            user_input = input(\"> \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                print(\"‚ö†Ô∏è  Please enter a valid question.\")\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"\\nüëã Thank you for using RAG HR Assistant. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if user_input.lower() == \"help\":\n",
    "                display_help()\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == \"history\":\n",
    "                history.display_history()\n",
    "                continue\n",
    "            \n",
    "            # Process query\n",
    "            result = run_hr_rag_session(user_input)\n",
    "            history.add_query(user_input, result)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"‚úÖ FINAL HR-READY ANSWER\")\n",
    "            print(\"=\"*80)\n",
    "            print(result[\"final_answer\"])\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"üìä REASONING TRACE\")\n",
    "            print(\"=\"*80)\n",
    "            print(format_trace_for_display(result[\"trace\"]))\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Session interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Unexpected error: {str(e)}\")\n",
    "            print(f\"‚ùå An error occurred: {str(e)}\")\n",
    "            print(\"Please try again with a different query.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Uncomment the line below to run a single demo query\n",
    "        # single_run_demo()\n",
    "        \n",
    "        # Run interactive query interface\n",
    "        interactive_loop()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Fatal error: {str(e)}\")\n",
    "        print(f\"‚ùå Fatal error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0284d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
